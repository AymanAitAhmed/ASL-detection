{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2KKf6xah_Kl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
        "\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from operator import add\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import torch.utils.data as data_utl\n",
        "import numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjFal8eKrIV"
      },
      "source": [
        "# Using the I3D model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUw6_c_CiNXd"
      },
      "outputs": [],
      "source": [
        "class Identity(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self.stride[dim] == 0:\n",
        "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
        "        else:\n",
        "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        return super(MaxPool3dSamePadding, self).forward(x)\n",
        "\n",
        "\n",
        "class Unit3D(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels,\n",
        "                 output_channels,\n",
        "                 kernel_shape=(1, 1, 1),\n",
        "                 stride=(1, 1, 1),\n",
        "                 padding=0,\n",
        "                 activation_fn=F.relu,\n",
        "                 use_batch_norm=True,\n",
        "                 use_bias=False,\n",
        "                 name='unit_3d'):\n",
        "\n",
        "        \"\"\"Initializes Unit3D module.\"\"\"\n",
        "        super(Unit3D, self).__init__()\n",
        "\n",
        "        self._output_channels = output_channels\n",
        "        self._kernel_shape = kernel_shape\n",
        "        self._stride = stride\n",
        "        self._use_batch_norm = use_batch_norm\n",
        "        self._activation_fn = activation_fn\n",
        "        self._use_bias = use_bias\n",
        "        self.name = name\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
        "                                out_channels=self._output_channels,\n",
        "                                kernel_size=self._kernel_shape,\n",
        "                                stride=self._stride,\n",
        "                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
        "                                bias=self._use_bias)\n",
        "\n",
        "        if self._use_batch_norm:\n",
        "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
        "\n",
        "    def compute_pad(self, dim, s):\n",
        "        if s % self._stride[dim] == 0:\n",
        "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
        "        else:\n",
        "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute 'same' padding\n",
        "        (batch, channel, t, h, w) = x.size()\n",
        "        #print t,h,w\n",
        "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
        "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
        "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
        "        #print out_t, out_h, out_w\n",
        "        pad_t = self.compute_pad(0, t)\n",
        "        pad_h = self.compute_pad(1, h)\n",
        "        pad_w = self.compute_pad(2, w)\n",
        "        #print pad_t, pad_h, pad_w\n",
        "\n",
        "        pad_t_f = pad_t // 2\n",
        "        pad_t_b = pad_t - pad_t_f\n",
        "        pad_h_f = pad_h // 2\n",
        "        pad_h_b = pad_h - pad_h_f\n",
        "        pad_w_f = pad_w // 2\n",
        "        pad_w_b = pad_w - pad_w_f\n",
        "\n",
        "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
        "        #print x.size()\n",
        "        #print pad\n",
        "        x = F.pad(x, pad)\n",
        "        #print x.size()\n",
        "\n",
        "        x = self.conv3d(x)\n",
        "        if self._use_batch_norm:\n",
        "            x = self.bn(x)\n",
        "        if self._activation_fn is not None:\n",
        "            x = self._activation_fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, name):\n",
        "        super(InceptionModule, self).__init__()\n",
        "\n",
        "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
        "                         name=name+'/Branch_0/Conv3d_0a_1x1')\n",
        "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_1/Conv3d_0a_1x1')\n",
        "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_1/Conv3d_0b_3x3')\n",
        "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_2/Conv3d_0a_1x1')\n",
        "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
        "                          name=name+'/Branch_2/Conv3d_0b_3x3')\n",
        "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
        "                                stride=(1, 1, 1), padding=0)\n",
        "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
        "                          name=name+'/Branch_3/Conv3d_0b_1x1')\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        b0 = self.b0(x)\n",
        "        b1 = self.b1b(self.b1a(x))\n",
        "        b2 = self.b2b(self.b2a(x))\n",
        "        b3 = self.b3b(self.b3a(x))\n",
        "        return torch.cat([b0,b1,b2,b3], dim=1)\n",
        "\n",
        "\n",
        "class InceptionI3d(nn.Module):\n",
        "    \"\"\"Inception-v1 I3D architecture.\n",
        "    The model is introduced in:\n",
        "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
        "        Joao Carreira, Andrew Zisserman\n",
        "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
        "    See also the Inception architecture, introduced in:\n",
        "        Going deeper with convolutions\n",
        "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
        "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
        "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
        "    \"\"\"\n",
        "\n",
        "    # Endpoints of the model in order. During construction, all the endpoints up\n",
        "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
        "    # second return value.\n",
        "    VALID_ENDPOINTS = (\n",
        "        'Conv3d_1a_7x7',\n",
        "        'MaxPool3d_2a_3x3',\n",
        "        'Conv3d_2b_1x1',\n",
        "        'Conv3d_2c_3x3',\n",
        "        'MaxPool3d_3a_3x3',\n",
        "        'Mixed_3b',\n",
        "        'Mixed_3c',\n",
        "        'MaxPool3d_4a_3x3',\n",
        "        'Mixed_4b',\n",
        "        'Mixed_4c',\n",
        "        'Mixed_4d',\n",
        "        'Mixed_4e',\n",
        "        'Mixed_4f',\n",
        "        'MaxPool3d_5a_2x2',\n",
        "        'Mixed_5b',\n",
        "        'Mixed_5c',\n",
        "        'Logits',\n",
        "        'Predictions',\n",
        "    )\n",
        "\n",
        "    def __init__(self, num_classes=400, spatial_squeeze=True,\n",
        "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n",
        "        \"\"\"Initializes I3D model instance.\n",
        "        Args:\n",
        "          num_classes: The number of outputs in the logit layer (default 400, which\n",
        "              matches the Kinetics dataset).\n",
        "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
        "              before returning (default True).\n",
        "          final_endpoint: The model contains many possible endpoints.\n",
        "              `final_endpoint` specifies the last endpoint for the model to be built\n",
        "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
        "              at endpoints up to `final_endpoint` will also be returned, in a\n",
        "              dictionary. `final_endpoint` must be one of\n",
        "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
        "          name: A string (optional). The name of this module.\n",
        "        Raises:\n",
        "          ValueError: if `final_endpoint` is not recognized.\n",
        "        \"\"\"\n",
        "\n",
        "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
        "\n",
        "        super(InceptionI3d, self).__init__()\n",
        "        self._num_classes = num_classes\n",
        "        self._spatial_squeeze = spatial_squeeze\n",
        "        self._final_endpoint = final_endpoint\n",
        "        self.logits = None\n",
        "\n",
        "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
        "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
        "\n",
        "        self.end_points = {}\n",
        "        end_point = 'Conv3d_1a_7x7'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
        "                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_2a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2b_1x1'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Conv3d_2c_3x3'\n",
        "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
        "                                       name=name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_3a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3b'\n",
        "        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_3c'\n",
        "        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_4a_3x3'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4b'\n",
        "        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4c'\n",
        "        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4d'\n",
        "        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4e'\n",
        "        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_4f'\n",
        "        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'MaxPool3d_5a_2x2'\n",
        "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
        "                                                             padding=0)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5b'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Mixed_5c'\n",
        "        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n",
        "        if self._final_endpoint == end_point: return\n",
        "\n",
        "        end_point = 'Logits'\n",
        "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
        "                                     stride=(1, 1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
        "        self.logits  = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "\n",
        "        self.build()\n",
        "\n",
        "\n",
        "    def replace_logits(self, num_classes):\n",
        "        self._num_classes = num_classes\n",
        "        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n",
        "                             kernel_shape=[1, 1, 1],\n",
        "                             padding=0,\n",
        "                             activation_fn=None,\n",
        "                             use_batch_norm=False,\n",
        "                             use_bias=True,\n",
        "                             name='logits')\n",
        "    def remove_last(self):\n",
        "        self.logits = Identity()\n",
        "\n",
        "    def build(self):\n",
        "        for k in self.end_points.keys():\n",
        "            self.add_module(k, self.end_points[k])\n",
        "\n",
        "    def forward(self, x, pretrained=False, n_tune_layers=-1):\n",
        "        if pretrained:\n",
        "            assert n_tune_layers >= 0\n",
        "\n",
        "            freeze_endpoints = self.VALID_ENDPOINTS[:-n_tune_layers]\n",
        "            tune_endpoints = self.VALID_ENDPOINTS[-n_tune_layers:]\n",
        "        else:\n",
        "            freeze_endpoints = []\n",
        "            tune_endpoints = self.VALID_ENDPOINTS\n",
        "\n",
        "        # backbone, no gradient part\n",
        "        with torch.no_grad():\n",
        "            for end_point in freeze_endpoints:\n",
        "                if end_point in self.end_points:\n",
        "                    x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
        "\n",
        "        # backbone, gradient part\n",
        "        for end_point in tune_endpoints:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x) # use _modules to work with dataparallel\n",
        "\n",
        "        # head\n",
        "        x = self.logits(self.dropout(self.avg_pool(x)))\n",
        "        if self._spatial_squeeze:\n",
        "            logits = x.squeeze(3).squeeze(3)\n",
        "        # logits is batch X time X classes, which is what we want to work with\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        for end_point in self.VALID_ENDPOINTS:\n",
        "            if end_point in self.end_points:\n",
        "                x = self._modules[end_point](x)\n",
        "        return self.avg_pool(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ulB5quiSia"
      },
      "outputs": [],
      "source": [
        "i3d = InceptionI3d(400, in_channels=3)\n",
        "i3d.replace_logits(2731)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7f8KcueJCT3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def download_weights(link, filename):\n",
        "\n",
        "\n",
        "    # Send a GET request to download the file\n",
        "    response = requests.get(link)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Open a local file to write the content\n",
        "        with open(f\"{filename}.zip\", \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"File downloaded successfully!\")\n",
        "    else:\n",
        "        print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXjPb-CTiYJ6"
      },
      "outputs": [],
      "source": [
        "# Define the URL of the file in the GitHub release\n",
        "github_release_url = \"https://github.com/microsoft/ASL-citizen-code/releases/download/checkpoints_v1/ASL_citizen_I3D_weights.zip\"\n",
        "download_weights(github_release_url, 'I3D_weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM-I2znYiaEa"
      },
      "outputs": [],
      "source": [
        "!unzip -d /content/ /content/I3D_weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBr1tVnaij9p"
      },
      "outputs": [],
      "source": [
        "i3d.load_state_dict(torch.load('/content/ASL_citizen_I3D_weights.pt'))\n",
        "i3d.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lk-bFclisoJ"
      },
      "outputs": [],
      "source": [
        "#loads rgb frames from video path, centering and downsizing as needed\n",
        "def load_rgb_frames_from_video(video_path, max_frames=64, resize=(256, 256)):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(\"total frameS:\",total_frames)\n",
        "\n",
        "    start = 0\n",
        "    frameskip = 1\n",
        "\n",
        "    # Adjust FPS dynamically based on length of video\n",
        "    frameskip = 1\n",
        "    if total_frames >= 96:\n",
        "        frameskip = 2\n",
        "    if total_frames >= 160:\n",
        "        frameskip = 3\n",
        "\n",
        "    # Set start frame so the video is \"centered\" across frames\n",
        "    if frameskip == 3:\n",
        "        start = np.clip(int((total_frames - 192) // 2), 0, 160)\n",
        "    elif frameskip == 2:\n",
        "        start = np.clip(int((total_frames - 128) // 2), 0, 96)\n",
        "    else:\n",
        "        start = np.clip(int((total_frames - 64) // 2), 0, 64)\n",
        "    vidcap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    for offset in range(0, min(max_frames * frameskip, int(total_frames - start))):\n",
        "        success, img = vidcap.read()\n",
        "        if offset % frameskip == 0:\n",
        "            w, h, c = img.shape\n",
        "            if w < 226 or h < 226:\n",
        "                d = 226. - min(w, h)\n",
        "                sc = 1 + d / min(w, h)\n",
        "                img = cv2.resize(img, dsize=(0, 0), fx=sc, fy=sc)\n",
        "            if w > 256 or h > 256:\n",
        "                img = cv2.resize(img, (math.ceil(w * (256 / w)), math.ceil(h * (256 / h))))\n",
        "            img = (img / 255.) * 2 - 1\n",
        "            frames.append(img)\n",
        "    return np.asarray(frames, dtype=np.float32)\n",
        "\n",
        "def video_to_tensor(pic):\n",
        "    \"\"\"Convert a ``numpy.ndarray`` to tensor.\n",
        "    Converts a numpy.ndarray (T x H x W x C)\n",
        "    to a torch.FloatTensor of shape (C x T x H x W)\n",
        "\n",
        "    Args:\n",
        "         pic (numpy.ndarray): Video to be converted to tensor.\n",
        "    Returns:\n",
        "         Tensor: Converted video.\n",
        "    \"\"\"\n",
        "    return torch.from_numpy(pic.transpose([3, 0, 1, 2]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZuCApxSizsA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop the given video sequences (t x h x w) at a random location.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_params(img, output_size):\n",
        "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "            output_size (tuple): Expected output size of the crop.\n",
        "        Returns:\n",
        "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
        "        \"\"\"\n",
        "        t, h, w, c = img.shape\n",
        "        th, tw = output_size\n",
        "        if w == tw and h == th:\n",
        "            return 0, 0, h, w\n",
        "\n",
        "        i = random.randint(0, h - th) if h!=th else 0\n",
        "        j = random.randint(0, w - tw) if w!=tw else 0\n",
        "        return i, j, th, tw\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "\n",
        "        i, j, h, w = self.get_params(imgs, self.size)\n",
        "\n",
        "        imgs = imgs[:, i:i+h, j:j+w, :]\n",
        "        return imgs\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
        "\n",
        "class CenterCrop(object):\n",
        "    \"\"\"Crops the given seq Images at the center.\n",
        "    Args:\n",
        "        size (sequence or int): Desired output size of the crop. If size is an\n",
        "            int instead of sequence like (h, w), a square crop (size, size) is\n",
        "            made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        if isinstance(size, numbers.Number):\n",
        "            self.size = (int(size), int(size))\n",
        "        else:\n",
        "            self.size = size\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image): Image to be cropped.\n",
        "        Returns:\n",
        "            PIL Image: Cropped image.\n",
        "        \"\"\"\n",
        "        t, h, w, c = imgs.shape\n",
        "        th, tw = self.size\n",
        "        i = int(np.round((h - th) / 2.))\n",
        "        j = int(np.round((w - tw) / 2.))\n",
        "\n",
        "        return imgs[:, i:i+th, j:j+tw, :]\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    \"\"\"Horizontally flip the given seq Images randomly with a given probability.\n",
        "    Args:\n",
        "        p (float): probability of the image being flipped. Default value is 0.5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (seq Images): seq Images to be flipped.\n",
        "        Returns:\n",
        "            seq Images: Randomly flipped seq images.\n",
        "        \"\"\"\n",
        "        if random.random() < self.p:\n",
        "            # t x h x w\n",
        "            return np.flip(imgs, axis=2).copy()\n",
        "        return imgs\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(p={})'.format(self.p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzDIOLhckMcp"
      },
      "outputs": [],
      "source": [
        "def pad(imgs, total_frames):\n",
        "        if imgs.shape[0] < total_frames:\n",
        "            num_padding = total_frames - imgs.shape[0]\n",
        "            if num_padding:\n",
        "                prob = np.random.random_sample()\n",
        "                if prob > 0.5: #pad with first frame\n",
        "                    pad_img = imgs[0]\n",
        "                    pad = np.tile(np.expand_dims(pad_img, axis=0), (num_padding, 1, 1, 1))\n",
        "                    padded_imgs = np.concatenate([imgs, pad], axis=0)\n",
        "                else: #pad with last frame\n",
        "                    pad_img = imgs[-1]\n",
        "                    pad = np.tile(np.expand_dims(pad_img, axis=0), (num_padding, 1, 1, 1))\n",
        "                    padded_imgs = np.concatenate([imgs, pad], axis=0)\n",
        "        else:\n",
        "            padded_imgs = imgs\n",
        "        return padded_imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xat67R8TK8Ia"
      },
      "source": [
        "# Using the ST-GCN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEyLHRC8LA0L"
      },
      "outputs": [],
      "source": [
        "def get_hop_distance(num_node, edge, max_hop=1):\n",
        "    # link matrix\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in edge:\n",
        "        A[j, i] = 1\n",
        "        A[i, j] = 1\n",
        "\n",
        "    # compute hop steps\n",
        "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
        "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n",
        "    arrive_mat = np.stack(transfer_mat) > 0\n",
        "    for d in range(max_hop, -1, -1):\n",
        "        hop_dis[arrive_mat[d]] = d\n",
        "    return hop_dis\n",
        "\n",
        "\n",
        "def normalize_digraph(A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i] ** (-1)\n",
        "    AD = np.dot(A, Dn)\n",
        "    return AD\n",
        "\n",
        "\n",
        "def edge2mat(link, num_node):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in link:\n",
        "        A[j, i] = 1\n",
        "    return A\n",
        "\n",
        "\n",
        "def get_spatial_graph(num_node, self_link, inward, outward):\n",
        "    I = edge2mat(self_link, num_node)\n",
        "    In = normalize_digraph(edge2mat(inward, num_node))\n",
        "    Out = normalize_digraph(edge2mat(outward, num_node))\n",
        "    A = np.stack((I, In, Out))\n",
        "    return A\n",
        "\n",
        "\n",
        "class GraphWithPartition:  # Unidirected, connections with hop limit\n",
        "    \"\"\"The Graph to model the skeletons\n",
        "    Args:\n",
        "        num_nodes (int): Number of spatial nodes in the graph.\n",
        "        center (int): Index of the center node.\n",
        "        inward_edges (list): List of spatial edges connecting the skeleton.\n",
        "        strategy (string): must be one of the follow candidates\n",
        "        - uniform: Uniform Labeling\n",
        "        - distance: Distance Partitioning\n",
        "        - spatial: Spatial Configuration\n",
        "        For more information, please refer to the section 'Partition\n",
        "        Strategies' in the ST-GCN paper (https://arxiv.org/abs/1801.07455).\n",
        "\n",
        "        max_hop (int): the maximal distance between two connected nodes. Default: 1\n",
        "        dilation (int): controls the spacing between the kernel points. Default: 1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_nodes,\n",
        "            center,\n",
        "            inward_edges,\n",
        "            strategy=\"spatial\",\n",
        "            max_hop=1,\n",
        "            dilation=1,\n",
        "    ):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.center = center\n",
        "        self.self_edges = [[i, i] for i in range(self.num_nodes)]\n",
        "        self.inward_edges = inward_edges\n",
        "        self.edges = self.self_edges + self.inward_edges\n",
        "\n",
        "        self.max_hop = max_hop\n",
        "        self.dilation = dilation\n",
        "\n",
        "        self.hop_dis = get_hop_distance(self.num_nodes, self.edges, max_hop=max_hop)\n",
        "        self.get_adjacency(strategy)\n",
        "\n",
        "    def get_adjacency(self, strategy):\n",
        "        valid_hop = range(0, self.max_hop + 1, self.dilation)\n",
        "        adjacency = np.zeros((self.num_nodes, self.num_nodes))\n",
        "        for hop in valid_hop:\n",
        "            adjacency[self.hop_dis == hop] = 1\n",
        "        normalize_adjacency = normalize_digraph(adjacency)\n",
        "\n",
        "        if strategy == \"uniform\":\n",
        "            A = np.zeros((1, self.num_nodes, self.num_nodes))\n",
        "            A[0] = normalize_adjacency\n",
        "            self.A = A\n",
        "        elif strategy == \"distance\":\n",
        "            A = np.zeros((len(valid_hop), self.num_nodes, self.num_nodes))\n",
        "            for i, hop in enumerate(valid_hop):\n",
        "                A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
        "            self.A = A\n",
        "        elif strategy == \"spatial\":\n",
        "            A = []\n",
        "            for hop in valid_hop:\n",
        "                a_root = np.zeros((self.num_nodes, self.num_nodes))\n",
        "                a_close = np.zeros((self.num_nodes, self.num_nodes))\n",
        "                a_further = np.zeros((self.num_nodes, self.num_nodes))\n",
        "                for i in range(self.num_nodes):\n",
        "                    for j in range(self.num_nodes):\n",
        "                        if self.hop_dis[j, i] == hop:\n",
        "                            if (\n",
        "                                    self.hop_dis[j, self.center]\n",
        "                                    == self.hop_dis[i, self.center]\n",
        "                            ):\n",
        "                                a_root[j, i] = normalize_adjacency[j, i]\n",
        "                            elif (\n",
        "                                    self.hop_dis[j, self.center]\n",
        "                                    > self.hop_dis[i, self.center]\n",
        "                            ):\n",
        "                                a_close[j, i] = normalize_adjacency[j, i]\n",
        "                            else:\n",
        "                                a_further[j, i] = normalize_adjacency[j, i]\n",
        "                if hop == 0:\n",
        "                    A.append(a_root)\n",
        "                else:\n",
        "                    A.append(a_root + a_close)\n",
        "                    A.append(a_further)\n",
        "            A = np.stack(A)\n",
        "            self.A = A\n",
        "        else:\n",
        "            raise ValueError(\"This Graph construction strategy is not supported\")\n",
        "\n",
        "\n",
        "class SpatialGraph:\n",
        "    \"\"\"\n",
        "    Graph construction with equal weight to all the nodes.\n",
        "    Args:\n",
        "        num_nodes (int): Number of spatial nodes in the graph.\n",
        "        inward_edges (list): List of spatial edges connecting the skeleton.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_nodes, inward_edges, strategy=\"spatial\"):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.strategy = strategy\n",
        "        self.self_edges = [(i, i) for i in range(num_nodes)]\n",
        "        self.inward_edges = inward_edges\n",
        "        self.outward_edges = [(j, i) for (i, j) in self.inward_edges]\n",
        "        self.A = self.get_adjacency_matrix()\n",
        "\n",
        "    def get_adjacency_matrix(self):\n",
        "        if self.strategy == \"spatial\":\n",
        "            return get_spatial_graph(\n",
        "                self.num_nodes, self.self_edges, self.inward_edges, self.outward_edges\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_G_-fYsLjJz"
      },
      "outputs": [],
      "source": [
        "class ConvTemporalGraphical(nn.Module):\n",
        "    \"\"\"The basic module for applying a graph convolution.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input sequence data.\n",
        "        out_channels (int): Number of channels produced by the convolution.\n",
        "        kernel_size (int): Size of the graph convolving kernel.\n",
        "        t_kernel_size (int): Size of the temporal convolving kernel.\n",
        "        t_stride (int, optional): Stride of the temporal convolution. Default: 1.\n",
        "        t_padding (int, optional): Temporal zero-padding added to both sides\n",
        "            of the input. Default: 0.\n",
        "        t_dilation (int, optional): Spacing between temporal kernel elements.\n",
        "            Default: 1.\n",
        "        bias (bool, optional): If ``True``, adds a learnable bias to the\n",
        "            output. Default: ``True``.\n",
        "    Shape:\n",
        "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)`\n",
        "            format\n",
        "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
        "        - Output[0]: Output graph sequence in :math:`(N, out_channels, T_{out}\n",
        "            , V)` format\n",
        "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V, V)\n",
        "            ` format\n",
        "        where\n",
        "            :math:`N` is a batch size,\n",
        "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]\n",
        "                `,\n",
        "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
        "            :math:`V` is the number of graph nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            t_kernel_size=1,\n",
        "            t_stride=1,\n",
        "            t_padding=0,\n",
        "            t_dilation=1,\n",
        "            bias=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels * kernel_size,\n",
        "            kernel_size=(t_kernel_size, 1),\n",
        "            padding=(t_padding, 0),\n",
        "            stride=(t_stride, 1),\n",
        "            dilation=(t_dilation, 1),\n",
        "            bias=bias,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, A):\n",
        "        assert A.size(0) == self.kernel_size\n",
        "\n",
        "        x = self.conv(x)\n",
        "        n, kc, t, v = x.size()\n",
        "        x = x.view(n, self.kernel_size, kc // self.kernel_size, t, v)\n",
        "        x = torch.einsum(\"nkctv,kvw->nctw\", (x, A))\n",
        "\n",
        "        return x.contiguous(), A\n",
        "\n",
        "\n",
        "class STGCN_BLOCK(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies a spatial temporal graph convolution over an input graph\n",
        "    sequence.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input sequence data.\n",
        "        out_channels (int): Number of channels produced by the convolution.\n",
        "        kernel_size (tuple): Size of the temporal convolving kernel and\n",
        "            graph convolving kernel.\n",
        "        stride (int, optional): Stride of the temporal convolution. Default: 1.\n",
        "        dropout (int, optional): Dropout rate of the final output. Default: 0.\n",
        "        residual (bool, optional): If ``True``, applies a residual mechanism. Default: ``True``.\n",
        "    Shape:\n",
        "        - Input[0]: Input graph sequence in :math:`(N, in_channels, T_{in}, V)`\n",
        "            format.\n",
        "        - Input[1]: Input graph adjacency matrix in :math:`(K, V, V)` format\n",
        "        - Output[0]: Output graph sequence in :math:`(N, out_channels, T_{out},\n",
        "            V)` format.\n",
        "        - Output[1]: Graph adjacency matrix for output data in :math:`(K, V,\n",
        "            V)` format.\n",
        "        where\n",
        "            :math:`N` is a batch size,\n",
        "            :math:`K` is the spatial kernel size, as :math:`K == kernel_size[1]`,\n",
        "            :math:`T_{in}/T_{out}` is a length of input/output sequence,\n",
        "            :math:`V` is the number of graph nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self, in_channels, out_channels, kernel_size, stride=1, dropout=0, residual=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(kernel_size) == 2\n",
        "        assert kernel_size[0] % 2 == 1\n",
        "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
        "\n",
        "        self.gcn = ConvTemporalGraphical(in_channels, out_channels, kernel_size[1])\n",
        "\n",
        "        self.tcn = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(\n",
        "                out_channels,\n",
        "                out_channels,\n",
        "                (kernel_size[0], 1),\n",
        "                (stride, 1),\n",
        "                padding,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.Dropout(dropout, inplace=True),\n",
        "        )\n",
        "\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "\n",
        "        else:\n",
        "            self.residual = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride, 1)),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, A):\n",
        "        res = self.residual(x)\n",
        "        x, A = self.gcn(x, A)\n",
        "        x = self.tcn(x) + res\n",
        "\n",
        "        return self.relu(x), A\n",
        "\n",
        "\n",
        "class STGCN(nn.Module):\n",
        "    \"\"\"Spatial temporal graph convolutional network backbone\n",
        "\n",
        "    This module is proposed in\n",
        "    `Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition\n",
        "    <https://arxiv.org/pdf/1801.07455.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input data.\n",
        "        graph_args (dict): The arguments for building the graph.\n",
        "        edge_importance_weighting (bool): If ``True``, adds a learnable importance weighting to the edges of the graph. Default: True.\n",
        "        n_out_features (int): Output Embedding dimension. Default: 256.\n",
        "        kwargs (dict): Other parameters for graph convolution units.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, graph_args, edge_importance_weighting, n_out_features=256, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.graph = GraphWithPartition(num_nodes=graph_args['num_nodes'], center=graph_args['center'],\n",
        "                                        inward_edges=graph_args['inward_edges'])\n",
        "        A = torch.tensor(self.graph.A, dtype=torch.float32, requires_grad=False)\n",
        "        self.register_buffer(\"A\", A)\n",
        "\n",
        "        spatial_kernel_size = A.size(0)\n",
        "        temporal_kernel_size = 9\n",
        "        self.n_out_features = n_out_features\n",
        "        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n",
        "        self.data_bn = nn.BatchNorm1d(in_channels * A.size(1))\n",
        "        kwargs0 = {k: v for k, v in kwargs.items() if k != \"dropout\"}\n",
        "        self.st_gcn_networks = nn.ModuleList(\n",
        "            (\n",
        "                STGCN_BLOCK(in_channels, 64, kernel_size, 1, residual=False, **kwargs0),\n",
        "                STGCN_BLOCK(64, 64, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(64, 64, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(64, 64, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(64, 128, kernel_size, 2, **kwargs),\n",
        "                STGCN_BLOCK(128, 128, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(128, 128, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(128, 256, kernel_size, 2, **kwargs),\n",
        "                STGCN_BLOCK(256, 256, kernel_size, 1, **kwargs),\n",
        "                STGCN_BLOCK(256, self.n_out_features, kernel_size, 1, **kwargs),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if edge_importance_weighting:\n",
        "            self.edge_importance = nn.ParameterList(\n",
        "                [nn.Parameter(torch.ones(self.A.size())) for i in self.st_gcn_networks]\n",
        "            )\n",
        "        else:\n",
        "            self.edge_importance = [1] * len(self.st_gcn_networks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape :math:`(N, in\\_channels, T_{in}, V_{in})`\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output embedding of shape :math:`(N, n\\_out\\_features)`\n",
        "\n",
        "        where\n",
        "            - :math:`N` is a batch size,\n",
        "            - :math:`T_{in}` is a length of input sequence,\n",
        "            - :math:`V_{in}` is the number of graph nodes,\n",
        "            - :math:`n\\_out\\_features` is the output embedding dimension.\n",
        "\n",
        "        \"\"\"\n",
        "        N, C, T, V = x.size()\n",
        "        x = x.permute(0, 3, 1, 2).contiguous()  # NCTV -> NVCT\n",
        "        x = x.view(N, V * C, T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N, V, C, T)\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()  # NVCT -> NCTV\n",
        "\n",
        "        for gcn, importance in zip(self.st_gcn_networks, self.edge_importance):\n",
        "            x, _ = gcn(x, self.A * importance)\n",
        "\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\n",
        "        x = x.view(N, -1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j79dkq9dLnp7"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4UbZBTfLpVa"
      },
      "outputs": [],
      "source": [
        "class FC(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully connected layer head\n",
        "    Args:\n",
        "        n_features (int): Number of features in the input.\n",
        "        num_class (int): Number of class for classification.\n",
        "        dropout_ratio (float): Dropout ratio to use. Default: 0.2.\n",
        "        batch_norm (bool): Whether to use batch norm or not. Default: ``False``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features, num_class, dropout_ratio=0.2, batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
        "        self.bn = batch_norm\n",
        "        if batch_norm:\n",
        "            self.bn = nn.BatchNorm1d(self.n_features)\n",
        "            self.bn.weight.data.fill_(1)\n",
        "            self.bn.bias.data.zero_()\n",
        "        self.classifier = nn.Linear(n_features, num_class)\n",
        "        nn.init.normal_(self.classifier.weight, 0, math.sqrt(2.0 / num_class))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape: (batch_size, n_features)\n",
        "\n",
        "        returns:\n",
        "            torch.Tensor: logits for classification.\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        if self.bn:\n",
        "            x = self.bn(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRquaLXVLrsa"
      },
      "outputs": [],
      "source": [
        "class Compose:\n",
        "    \"\"\"\n",
        "    Compose a list of pose transforms\n",
        "\n",
        "    Args:\n",
        "        transforms (list): List of transforms to be applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, x: dict):\n",
        "        \"\"\"Applies the given list of transforms\n",
        "\n",
        "        Args:\n",
        "            x (dict): input data\n",
        "\n",
        "        Returns:\n",
        "            dict: data after the transforms\n",
        "        \"\"\"\n",
        "        for transform in self.transforms:\n",
        "            x = transform(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Adopted from: https://github.com/AmitMY/pose-format/\n",
        "class ShearTransform:\n",
        "    \"\"\"\n",
        "    Applies `2D shear <https://en.wikipedia.org/wiki/Shear_matrix>`_ transformation\n",
        "\n",
        "    Args:\n",
        "        shear_std (float): std to use for shear transformation. Default: 0.2\n",
        "    \"\"\"\n",
        "    def __init__(self, shear_std: float=0.2):\n",
        "        self.shear_std = shear_std\n",
        "\n",
        "    def __call__(self, data:dict):\n",
        "        \"\"\"\n",
        "        Applies shear transformation to the given data.\n",
        "\n",
        "        Args:\n",
        "            data (dict): input data\n",
        "\n",
        "        Returns:\n",
        "            dict: data after shear transformation\n",
        "        \"\"\"\n",
        "\n",
        "        x = data\n",
        "        assert x.shape[0] == 2, \"Only 2 channels inputs supported for ShearTransform\"\n",
        "        x = x.permute(1, 2, 0) #CTV->TVC\n",
        "        shear_matrix = torch.eye(2)\n",
        "        shear_matrix[0][1] = torch.tensor(\n",
        "            np.random.normal(loc=0, scale=self.shear_std, size=1)[0])\n",
        "        res = torch.matmul(x.float(), shear_matrix.float())\n",
        "        data = res.permute(2, 0, 1) #TVC->CTV\n",
        "        return data.double()\n",
        "\n",
        "\n",
        "class RotatationTransform:\n",
        "    \"\"\"\n",
        "    Applies `2D rotation <https://en.wikipedia.org/wiki/Rotation_matrix>`_ transformation.\n",
        "\n",
        "    Args:\n",
        "        rotation_std (float): std to use for rotation transformation. Default: 0.2\n",
        "    \"\"\"\n",
        "    def __init__(self, rotation_std: float=0.2):\n",
        "        self.rotation_std = rotation_std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        \"\"\"\n",
        "        Applies rotation transformation to the given data.\n",
        "\n",
        "        Args:\n",
        "            data (dict): input data\n",
        "\n",
        "        Returns:\n",
        "            dict: data after rotation transformation\n",
        "        \"\"\"\n",
        "        x = data\n",
        "        assert x.shape[0] == 2, \"Only 2 channels inputs supported for RotationTransform\"\n",
        "        x = x.permute(1, 2, 0) #CTV->TVC\n",
        "        rotation_angle = torch.tensor(\n",
        "            np.random.normal(loc=0, scale=self.rotation_std, size=1)[0]\n",
        "        )\n",
        "        rotation_cos = torch.cos(rotation_angle)\n",
        "        rotation_sin = torch.sin(rotation_angle)\n",
        "        rotation_matrix = torch.tensor(\n",
        "            [[rotation_cos, -rotation_sin], [rotation_sin, rotation_cos]],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        res = torch.matmul(x.float(), rotation_matrix.float())\n",
        "        data = res.permute(2, 0, 1) #TVC->CTV\n",
        "        return data.double()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeY6ZwbrLwVS"
      },
      "outputs": [],
      "source": [
        "# Define the URL of the file in the GitHub release\n",
        "github_release_url = \"https://github.com/microsoft/ASL-citizen-code/releases/download/checkpoints_v1/ASL_citizen_stgcn_weights.zip\"\n",
        "download_weights(github_release_url, 'stgcn_weights')\n",
        "!!unzip -d /content/ /content/stgcn_weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sK2h5OYNu3k"
      },
      "outputs": [],
      "source": [
        "with open('/content/gloss_dict.json') as f:\n",
        "    gloss2idx = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWM8I7xCNwy0"
      },
      "outputs": [],
      "source": [
        "idx2gloss = {}\n",
        "for g in gloss2idx:\n",
        "    idx = gloss2idx[g]\n",
        "    idx2gloss[idx] = g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFpu11I8Mais"
      },
      "outputs": [],
      "source": [
        "#load model\n",
        "n_features = 256\n",
        "n_classes = len(gloss2idx)\n",
        "graph_args = {'num_nodes': 27, 'center': 0,\n",
        "              'inward_edges': [[2, 0], [1, 0], [0, 3], [0, 4], [3, 5],\n",
        "                               [4, 6], [5, 7], [6, 17], [7, 8], [7, 9],\n",
        "                               [9, 10], [7, 11], [11, 12], [7, 13], [13, 14],\n",
        "                               [7, 15], [15, 16], [17, 18], [17, 19], [19, 20],\n",
        "                               [17, 21], [21, 22], [17, 23], [23, 24], [17, 25], [25, 26]]}\n",
        "stgcn = STGCN(in_channels=2, graph_args=graph_args, edge_importance_weighting=True)\n",
        "fc = FC(n_features=n_features, num_class=n_classes, dropout_ratio=0.05)\n",
        "pose_model = Network(encoder=stgcn, decoder=fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVjjhig9MbZi"
      },
      "outputs": [],
      "source": [
        "pose_model.load_state_dict(torch.load('/content/ASL_citizen_stgcn_weights.pt'))\n",
        "pose_model.cuda()\n",
        "pose_model.train(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l48Lvw_8MkHq"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJrC5dzIMnr7"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "def extract_mediapipe_keypoints(video_path, min_detection_confidence=0.5):\n",
        "\n",
        "    # Initialize MediaPipe Holistic.\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    holistic = mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=min_detection_confidence\n",
        "    )\n",
        "\n",
        "    # Initialize video capture.\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    if not video.isOpened():\n",
        "        holistic.close()\n",
        "        raise ValueError(f\"Unable to open video file: {video_path}\")\n",
        "\n",
        "    # Get total number of frames.\n",
        "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames <= 0:\n",
        "        holistic.close()\n",
        "        video.release()\n",
        "        raise ValueError(f\"Video has no frames or cannot determine frame count: {video_path}\")\n",
        "\n",
        "    # Initialize the feature array.\n",
        "    # 543 keypoints: 33 pose + 21 right hand + 21 left hand + 468 face\n",
        "    feature =  np.zeros((int(total_frames), 543, 2))\n",
        "\n",
        "    count = 0\n",
        "    while count < total_frames:\n",
        "        success, image = video.read()\n",
        "        if not success:\n",
        "            break  # Exit if no more frames are available.\n",
        "\n",
        "        # Convert the BGR image to RGB.\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Process the image and extract landmarks.\n",
        "        results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        for i in range(33):\n",
        "           if results.pose_landmarks:\n",
        "                feature[count][i][0] = results.pose_landmarks.landmark[i].x\n",
        "                feature[count][i][1] = results.pose_landmarks.landmark[i].y\n",
        "\n",
        "        j = 33\n",
        "        for i in range(21):\n",
        "            if results.right_hand_landmarks:\n",
        "               feature[count][i+j][0] = results.right_hand_landmarks.landmark[i].x\n",
        "               feature[count][i+j][1] = results.right_hand_landmarks.landmark[i].y\n",
        "\n",
        "        j = 54\n",
        "        for i in range(21):\n",
        "            if results.left_hand_landmarks:\n",
        "                feature[count][i+j][0] = results.left_hand_landmarks.landmark[i].x\n",
        "                feature[count][i+j][1] = results.left_hand_landmarks.landmark[i].y\n",
        "\n",
        "        j = 75\n",
        "        for i in range(468):\n",
        "            if results.face_landmarks:\n",
        "                feature[count][i+j][0] = results.face_landmarks.landmark[i].x\n",
        "                feature[count][i+j][1] = results.face_landmarks.landmark[i].y\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # Release resources.\n",
        "    holistic.close()\n",
        "    video.release()\n",
        "\n",
        "    # If the actual number of processed frames is less than total_frames,\n",
        "    # truncate the keypoints array accordingly.\n",
        "    if count < total_frames:\n",
        "        feature = feature[:count]\n",
        "\n",
        "    return feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xbpn0WvMtFj"
      },
      "outputs": [],
      "source": [
        "#downsamples set of frames to get max frames\n",
        "def downsample(frames, max_frames):\n",
        "    length = frames.shape[0]\n",
        "    # Adjust FPS dynamically based on length of video\n",
        "    increment = max_frames / length\n",
        "    if increment > 1.0:\n",
        "        increment = 1.0\n",
        "    curr_increment = 0\n",
        "    curr_frame = 0\n",
        "    new_frames = []\n",
        "    for f in frames:\n",
        "        curr_increment += increment\n",
        "        if curr_increment > curr_frame:\n",
        "            curr_frame += 1\n",
        "            new_frames.append(f)\n",
        "    if len(new_frames) > max_frames:\n",
        "        new_frames = new_frames[:max_frames]\n",
        "    return np.array(new_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT_YqWmNMvuc"
      },
      "outputs": [],
      "source": [
        "def preprocess_stgcn(video_path):\n",
        "\n",
        "    max_frames = 128\n",
        "    #load frames and downsample / pad as needed\n",
        "    data0 = extract_mediapipe_keypoints(video_path)\n",
        "    length = data0.shape[0]\n",
        "    if length > max_frames:\n",
        "        data0 = downsample(data0, max_frames)\n",
        "    if length < max_frames:\n",
        "        data0 = np.pad(data0, ((0, max_frames - length), (0, 0), (0, 0)))\n",
        "\n",
        "    #normalize keypoints using distance between shoulders as reference\n",
        "    shoulder_l = data0[:, 11, :]\n",
        "    shoulder_r = data0[:, 12, :]\n",
        "\n",
        "    center = np.zeros(2)\n",
        "    for i in range(len(shoulder_l)):\n",
        "        center_i = (shoulder_r[i] + shoulder_l[i]) / 2\n",
        "        center = center + center_i\n",
        "    center = center / shoulder_l.shape[0]\n",
        "\n",
        "    mean_dist = np.mean(np.sqrt(((shoulder_l - shoulder_r) ** 2).sum(-1)))\n",
        "    if mean_dist != 0:\n",
        "        scale = 1.0 / mean_dist\n",
        "        data0 = data0 - center\n",
        "        data0 = data0 * scale\n",
        "\n",
        "    #select subset of keypoints for graph\n",
        "    keypoints = [0, 2, 5, 11, 12, 13, 14, 33, 37, 38, 41, 42, 45, 46, 49, 50, 53, 54,\n",
        "                 58, 59, 62, 63, 66, 67, 70, 71, 74]\n",
        "    data0 = data0[:, 0:75, :]\n",
        "    posedata = data0[:, 0:33, :]\n",
        "    rhdata = data0[:, 33:54, :]\n",
        "    lhdata = data0[:, 54:, :]\n",
        "\n",
        "    data = np.concatenate([posedata, lhdata, rhdata], axis=1)\n",
        "    data = data[:, keypoints, :]\n",
        "    data = np.transpose(data, (2, 0, 1))\n",
        "\n",
        "    ret_img = torch.from_numpy(data)\n",
        "\n",
        "    return ret_img.unsqueeze(0).float().cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47fax_DmOGb7"
      },
      "source": [
        "# Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSAnWPyaM5CL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript,HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def record_video(filename):\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/mp4; codecs=avc1.42E01E, mp4a.40.2\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "\n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:false, video: true});\n",
        "\n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "\n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(filename,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{filename}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_video_class(video_path, model, total_frames=64):\n",
        "    \"\"\"\n",
        "    Predict the class label of a video.\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): Path to the video.\n",
        "    - model (torch.nn.Module): The pre-trained model (e.g., I3D).\n",
        "    - total_frames (int): Number of frames to use from the video (default is 64).\n",
        "\n",
        "    Returns:\n",
        "    - predicted_class (str): The predicted class label.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Load and preprocess the video frames\n",
        "    frames = load_rgb_frames_from_video(video_path, total_frames)\n",
        "    frames = pad(frames, total_frames)\n",
        "    inputs = video_to_tensor(frames).cuda()\n",
        "    print(inputs.unsqueeze(0).shape)\n",
        "    # Model inference (disable gradient calculation)\n",
        "    with torch.no_grad():\n",
        "        per_frame_logits = model(inputs.unsqueeze(0), pretrained=False)\n",
        "\n",
        "    # Apply softmax to get probabilities for each frame\n",
        "    predictions = torch.max(per_frame_logits, dim=2)[0]\n",
        "    y_pred_tag = torch.softmax(predictions, dim=1)\n",
        "\n",
        "    # Get the top prediction class\n",
        "    pred_class_index = torch.argmax(y_pred_tag, dim=1)[0][0][0].item()\n",
        "\n",
        "    # Map the indices to class labels\n",
        "    predicted_class = idx2gloss[pred_class_index]\n",
        "\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "-p5mw4nZWniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6-Jl6XtPHrS"
      },
      "outputs": [],
      "source": [
        "def predict_STGCN_from_webcam(filename='recorded_video.mp4', del_end = True):\n",
        "    \"\"\"\n",
        "    Records a video using the webcam, preprocesses it, and predicts the word using the ST-GCN model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained ST-GCN model.\n",
        "        label_map (list): List mapping class indices to words.\n",
        "        filename (str): Name of the file to save the recorded video.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted word.\n",
        "    \"\"\"\n",
        "    # Step 1: Record the video\n",
        "    record_video(filename)\n",
        "\n",
        "    # Step 2: Preprocess the video\n",
        "    input_tensor = preprocess_stgcn(filename)  # Ensure max_frames is passed if needed\n",
        "    print(input_tensor.shape)\n",
        "    # Step 4: Model prediction\n",
        "    pose_model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = pose_model(input_tensor)\n",
        "        y_pred_tag = torch.softmax(predictions, dim=1)\n",
        "        pred_args = torch.argsort(y_pred_tag, dim=1, descending=True)\n",
        "\n",
        "    id_pred = pred_args[0][0].item()\n",
        "    predicted_word = idx2gloss[id_pred]\n",
        "\n",
        "    print(f\"Predicted word: {predicted_word}\")\n",
        "    # Step 5: delete the video after prediction if del_end = True\n",
        "    if del_end:\n",
        "        os.remove(filename)\n",
        "\n",
        "    return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i3d.cuda()"
      ],
      "metadata": {
        "id": "QrQx1yo0lrlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_input = torch.randn(torch.Size([1, 3, 64, 256, 256])).cuda()\n",
        "onnx_program = torch.onnx.export(i3d, torch_input, 'i3d.onnx', verbose=True, opset_version=12, operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)"
      ],
      "metadata": {
        "id": "sZFXUG14MBa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjxhu63EQaEb"
      },
      "outputs": [],
      "source": [
        "predict_STGCN_from_webcam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jF1DMNJQLmD"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "def annotate_video(video_path, prediction, output_path='annotated_video.mp4'):\n",
        "    \"\"\"\n",
        "    Annotates the video with MediaPipe landmarks and the prediction text using uniform size and color.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the original video.\n",
        "        prediction (str): The predicted word to overlay.\n",
        "        output_path (str): Path to save the annotated video.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize MediaPipe Holistic.\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    # Define unified DrawingSpecs for landmarks and connections.\n",
        "    landmark_spec = mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)  # Green color\n",
        "    connection_spec = mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1)\n",
        "\n",
        "    # Open the original video.\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Unable to open video file: {video_path}\")\n",
        "\n",
        "    # Get video properties.\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Define the codec and create VideoWriter object.\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using 'mp4v' for MP4 format\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    with mp_holistic.Holistic(\n",
        "        static_image_mode=False,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    ) as holistic:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Convert the BGR image to RGB.\n",
        "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Process the image and extract landmarks.\n",
        "            results = holistic.process(image_rgb)\n",
        "\n",
        "            # Draw landmarks on the frame with unified specs.\n",
        "            if results.pose_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                    landmark_spec, connection_spec)\n",
        "            if results.left_hand_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                    landmark_spec, connection_spec)\n",
        "            if results.right_hand_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                    landmark_spec, connection_spec)\n",
        "            if results.face_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    frame, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                    landmark_spec, connection_spec)\n",
        "\n",
        "            # Overlay the prediction text on the frame.\n",
        "            cv2.putText(\n",
        "                frame, f\"Prediction: {prediction}\", (30, 50),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "            # Write the annotated frame to the output video.\n",
        "            out.write(frame)\n",
        "\n",
        "    # Release resources.\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"Annotated video saved at: {output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV6oSblFWWlM"
      },
      "outputs": [],
      "source": [
        "def record_and_predict(model, label_map, device='cpu', filename='recorded_video.mp4',\n",
        "                      annotated_filename='annotated_video.mp4'):\n",
        "    \"\"\"\n",
        "    Records a video using the webcam, processes it to extract landmarks, predicts the word,\n",
        "    overlays the landmarks and prediction on the video, and displays the annotated video.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained ST-GCN model.\n",
        "        label_map (list): List mapping class indices to words.\n",
        "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
        "        filename (str): Name of the file to save the recorded video.\n",
        "        annotated_filename (str): Name of the file to save the annotated video.\n",
        "        max_frames (int): Maximum number of frames to process.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted word.\n",
        "    \"\"\"\n",
        "    # Step 1: Record the video\n",
        "    record_video(filename)\n",
        "\n",
        "    i3d_pred = predict_video_class(filename,i3d)\n",
        "    print(\"the i3d model predicted:\", i3d_pred)\n",
        "\n",
        "    # Step 2: Preprocess the video\n",
        "    input_tensor = preprocess_stgcn(filename)  # Shape: (C, T, V)\n",
        "\n",
        "    # Step 3: Add batch dimension and move to device\n",
        "    input_tensor = input_tensor\n",
        "\n",
        "    # Step 4: Model prediction\n",
        "    pose_model.eval()  # Ensure the model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        predictions = pose_model(input_tensor)\n",
        "        y_pred_tag = torch.softmax(predictions, dim=1)\n",
        "        pred_args = torch.argsort(y_pred_tag, dim=1, descending=True)\n",
        "\n",
        "    id_pred = pred_args[0][0].item()\n",
        "\n",
        "    # Step 5: Map the prediction to the corresponding word\n",
        "    predicted_word = label_map[id_pred]\n",
        "\n",
        "    print(f\"ST-GCN model Predicted: {predicted_word}\")\n",
        "\n",
        "    # Step 6: Annotate the video with landmarks and prediction\n",
        "    annotate_video(filename, predicted_word, output_path=annotated_filename)\n",
        "\n",
        "\n",
        "    return predicted_word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "\n",
        "    def display_video(video_path):\n",
        "        \"\"\"\n",
        "        Displays a video in the Colab notebook.\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to the video file.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        mp4 = open(video_path,'rb').read()\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "        display(HTML(f\"\"\"\n",
        "            <video width=\"640\" height=\"480\" controls>\n",
        "                <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "            </video>\n",
        "        \"\"\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "ulI82ZcnrVB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i /content/final_video.mp4"
      ],
      "metadata": {
        "id": "6DVhbdc7t_6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG6X9WSxWYZ7"
      },
      "outputs": [],
      "source": [
        "true_label = \"THANK_YOU\"\n",
        "ypredicted_word = record_and_predict(\n",
        "    model=pose_model,\n",
        "    label_map=idx2gloss,\n",
        "    device=\"cuda\",\n",
        "    filename= true_label + '_recorded.mp4',\n",
        "    annotated_filename= true_label + '_annotated.mp4',\n",
        ")\n",
        "!ffmpeg -loglevel quiet -y -i /content/THANK_YOU_annotated.mp4 -c:v libx264 -c:a aac -strict experimental -b:a 192k /content/final_THANK_YOU_annotated.mp4\n",
        "display_video(\"/content/final_THANK_YOU_annotated.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_label = \"FOR\"\n",
        "ypredicted_word = record_and_predict(\n",
        "    model=pose_model,\n",
        "    label_map=idx2gloss,\n",
        "    device=\"cuda\",\n",
        "    filename= true_label + '_recorded.mp4',\n",
        "    annotated_filename= true_label + '_annotated.mp4',\n",
        ")\n",
        "!ffmpeg -loglevel quiet -y -i /content/FOR_annotated.mp4 -c:v libx264 -c:a aac -strict experimental -b:a 192k /content/final_FOR_annotated.mp4\n",
        "display_video(\"/content/final_FOR_annotated.mp4\")"
      ],
      "metadata": {
        "id": "D1S9pL299atp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_label = \"YOUR\"\n",
        "ypredicted_word = record_and_predict(\n",
        "    model=pose_model,\n",
        "    label_map=idx2gloss,\n",
        "    device=\"cuda\",\n",
        "    filename= true_label + '_recorded.mp4',\n",
        "    annotated_filename= true_label + '_annotated.mp4',\n",
        ")\n",
        "!ffmpeg -loglevel quiet -y -i /content/YOUR_annotated.mp4 -c:v libx264 -c:a aac -strict experimental -b:a 192k /content/final_YOUR_annotated.mp4\n",
        "display_video(\"/content/final_YOUR_annotated.mp4\")"
      ],
      "metadata": {
        "id": "oPDe2rZ_ZjFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_label = \"ATTENTION\"\n",
        "ypredicted_word = record_and_predict(\n",
        "    model=pose_model,\n",
        "    label_map=idx2gloss,\n",
        "    device=\"cuda\",\n",
        "    filename= true_label + '_recorded.mp4',\n",
        "    annotated_filename= true_label + '_annotated.mp4',\n",
        ")\n",
        "!ffmpeg -loglevel quiet -y -i /content/ATTENTION_annotated.mp4 -c:v libx264 -c:a aac -strict experimental -b:a 192k /content/final_ATTENTION_annotated.mp4\n",
        "display_video(\"/content/final_ATTENTION_annotated.mp4\")"
      ],
      "metadata": {
        "id": "MgSHRa4RaQuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFqrxqQXeK4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}